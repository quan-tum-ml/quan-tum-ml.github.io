<!DOCTYPE html>
<html lang="en" dir=>

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Weights are initialized to small random values
Optimization #  As stated before, the goal of the optimizer is to minimize the loss function. It does this by changing the parameters of the Quantum circuit or neural network. However, a key difference is that it is impossible to access the quantum state at intermediate points during computation. Any attempt to observe the full state of the system would disrupt its quantum character.">
<meta name="theme-color" content="#FFFFFF"><meta property="og:title" content="" />
<meta property="og:description" content="Weights are initialized to small random values
Optimization #  As stated before, the goal of the optimizer is to minimize the loss function. It does this by changing the parameters of the Quantum circuit or neural network. However, a key difference is that it is impossible to access the quantum state at intermediate points during computation. Any attempt to observe the full state of the system would disrupt its quantum character." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.qu.antum.ml/docs/Table-of-contents/9.-Optimization/" />

<title>9. Optimization | ‚öõÔ∏èHybrid Quantum Classical Machine LearningüîÆ</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/book.min.1205ebacf147e31d6bd6f74be1ef8d96f4bb2c4943c68e5a39b4e988a774a2b0.css" integrity="sha256-EgXrrPFH4x1r1vdL4e&#43;NlvS7LElDxo5aObTpiKd0orA=">
<script defer src="/en.search.min.4439ea178c1b4cc0e60b3b7045d3b5f544291fd14ebe78bd0a0159ccb72d9e34.js" integrity="sha256-RDnqF4wbTMDmCztwRdO19UQpH9FOvni9CgFZzLctnjQ="></script>

<script defer src="/sw.min.74a8bb07f0bee86d6bb9a2750f073f14d93c7e4512f28860370cfd879e9719b4.js" integrity="sha256-dKi7B/C&#43;6G1ruaJ1Dwc/FNk8fkUS8ohgNwz9h56XGbQ="></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body dir=>
  <input type="checkbox" class="hidden" id="menu-control" />
  <main class="container flex">
    <aside class="book-menu">
      
  <nav>
<h2 class="book-brand">
  <a href="/"><span>‚öõÔ∏èHybrid Quantum Classical Machine LearningüîÆ</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>











  



  
  
  
  

  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
    <span>Table of Contents</span>
  

          
  
  
  

  
  <ul>
    
      
        <li>
          
  
    <a href="/docs/Table-of-contents/0.-Preface/" class="">0. Preface</a>
  

        </li>
      
    
      
        <li>
          
  
    <a href="/docs/Table-of-contents/1.-Introduction/" class="">1. Introduction</a>
  

        </li>
      
    
      
        <li>
          
  
    <a href="/docs/Table-of-contents/2.-The-Data/" class="">2. the Data</a>
  

        </li>
      
    
      
        <li>
          
  
    <a href="/docs/Table-of-contents/3.-Architecture/" class="">3. Architecture</a>
  

        </li>
      
    
      
        <li>
          
  
    <a href="/docs/Table-of-contents/4.-Embedding/" class="">4. Embedding</a>
  

        </li>
      
    
      
        <li>
          
  
    <a href="/docs/Table-of-contents/5.-Parametrized-Quantum-Circuit/" class="">5. Parametrized Quantum Circuit</a>
  

        </li>
      
    
      
        <li>
          
  
    <a href="/docs/Table-of-contents/6.-Measurement/" class="">6. Measurement</a>
  

        </li>
      
    
      
        <li>
          
  
    <a href="/docs/Table-of-contents/7.-Mapping/" class="">7. Mapping</a>
  

        </li>
      
    
      
        <li>
          
  
    <a href="/docs/Table-of-contents/8.-Loss/" class="">8. Loss</a>
  

        </li>
      
    
      
        <li>
          
  
    <a href="/docs/Table-of-contents/9.-Optimization/" class="active">9. Optimization</a>
  

        </li>
      
    
      
        <li>
          
  
    <a href="/docs/Table-of-contents/ZZZ/" class="">10. Results</a>
  

        </li>
      
    
  </ul>
  

        </li>
      
    
  </ul>
  











  
<ul>
  
  <li>
    <a href="https://github.com/quan-tum-ml" target="_blank" rel="noopener">
        Github
      </a>
  </li>
  
</ul>






</nav>




  <script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script>


 
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>9. Optimization</strong>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
    <input type="checkbox" class="hidden" id="toc-control" />
    <aside class="hidden clearfix">
      
  <nav id="TableOfContents">
  <ul>
    <li><a href="#optimization">Optimization</a>
      <ul>
        <li><a href="#parameter-shift-rule">Parameter-shift rule</a></li>
        <li><a href="#spsa">SPSA</a></li>
        <li><a href="#stochastic-gradient-descent">(Stochastic) Gradient descent</a>
          <ul>
            <li><a href="#minibatches">Minibatches</a></li>
            <li><a href="#learning-rate-schedule">Learning rate schedule</a></li>
            <li><a href="#adam">ADAM</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#talk-is-cheap-show-me-the-code---linus-torvalds">Talk is cheap, show me the code - Linus Torvalds</a></li>
    <li><a href="#training-time">Training time</a></li>
  </ul>
</nav>


    </aside>
  
 
      </header>

      
      
  <article class="markdown"><p>Weights are initialized to small random values</p>
<h1 id="optimization">
  Optimization
  <a class="anchor" href="#optimization">#</a>
</h1>
<p>As stated before, the goal of the optimizer is to minimize the loss function. It does this by changing the parameters of the Quantum circuit or neural network.
However, a key  difference  is  that  it  is  impossible  to access the quantum state at intermediate points during computation. Any attempt to observe the full state of the system would disrupt its quantum character. Moreover, it is difficult to conceive a circuit learning algorithm that truly resembles backpropagation, as it would rely on storing  the  intermediate  state  of  the  network  during  computation.  Backpropagation is the gold standard algorithm for neural networks and can be described as a computationally  efficient  organization  of  the  chain  rule that allows gradient descent to work on large-scale models.</p>
<p>While backprop-like algorithms have been proposed ( Backwards Quantum Propagation of Phase errors (Baqprop) TODO: Source <a href="https://arxiv.org/abs/1806.09729">https://arxiv.org/abs/1806.09729</a> A Universal Training Algorithm for Quantum Deep Learning), here we will make use of a simpler approach.</p>
<h2 id="parameter-shift-rule">
  Parameter-shift rule
  <a class="anchor" href="#parameter-shift-rule">#</a>
</h2>
<p>Initially, the parameters are set to small, random values.
By evaluating or circuit &ldquo;a little to the left&rdquo; and &ldquo;a little to the right&rdquo; of each parameter, the gradient can be estimated locally (estimation, because while the parameter shift rule is exact for arbitrarily small deltas, we can only execute the circuit a finite amount of times):
<blockquote class="book-hint info">
  <p>parameter shift rule</p>
<link rel="stylesheet" href="/katex/katex.min.css" />
<script defer src="/katex/katex.min.js"></script>
<script defer src="/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><span>
  \( \frac{\partial L}{\partial \theta_{j}} \approx \frac{L(\boldsymbol{\theta}&#43;c \boldsymbol{\Delta})-L(\boldsymbol{\theta}-c \boldsymbol{\Delta})}{2 c \Delta_{j}} \)
</span>

</blockquote>

where ‚àÜ is a (small) hyperparameter and ej is the Carte-sian unit vector in the j direction.
Note that while this works, it is extremely inefficient: computes  anapproximate  gradient  vector with two  evaluations of the loss function <strong>PER PARAMETER</strong> times the specified numer of finite iterations (&ldquo;shots&rdquo;):</p>
<span>
  \( 2 * \#Params * \#Shots \)
</span>

<p>Surely, there is a more efficient way to do this?</p>
<h2 id="spsa">
  SPSA
  <a class="anchor" href="#spsa">#</a>
</h2>
<p><strong>S</strong>imultaneous <strong>p</strong>erturbation <strong>s</strong>tochastic <strong>a</strong>pproximation <strong>only requires two  evaluations of the loss function</strong>, times the number of iterations.</p>
<span>
  \( 2 * \#Shots \)
</span>

<blockquote class="book-hint info">
  <p>SPSA</p>
<span>
  \( \frac{\partial L}{\partial \theta_{j}} \approx \frac{L\left(\boldsymbol{\theta}&#43;\Delta \boldsymbol{e}_{j}\right)-L\left(\boldsymbol{\theta}-\Delta \boldsymbol{e}_{j}\right)}{2 \Delta} \)
</span>

</blockquote>

<p>where <span>
  \( \bm{\Delta} \)
</span>
  is  a  random  perturbation  vector  and <span>
  \(c\)
</span>
  is  a (small) hyperparameter. So essentially we do not evaluate the gradient for each dimension indepently, but joined.</p>
<p>There are cases when finite difference methods are ill-conditioned and unstable due to truncation and round-off  errors. This  is  one  of  the  reasons  why  machinelearning relies on the analytical gradient when possible, and it is often calculated with automatic differentiation scheme. Compared to finite difference and SPSA, the analytical gradient has the advantage of providing an unbiased estimator. TODO: Source</p>
<h2 id="stochastic-gradient-descent">
  (Stochastic) Gradient descent
  <a class="anchor" href="#stochastic-gradient-descent">#</a>
</h2>
<p>Now, that we have obtained a gradient vector through eighter method, we need to use it to update the circuit&rsquo;s parameters.
In the realm of classical optimization gradient descent and it&rsquo;s derived methods have proven to be efficient and powerfull at optimization tasks.</p>
<blockquote class="book-hint info">
  <p>Gradient descent update rule</p>
<span>
  \( \boldsymbol{\theta} \leftarrow \boldsymbol{\theta}-\eta \nabla_{\boldsymbol{\theta}} L \)
</span>

</blockquote>

<p>Where <span>
  \(\nabla_{\boldsymbol{\theta}} L\)
</span>
 is the gradient vector and <span>
  \(\eta\)
</span>
 is the learning rate.
This update rule is then applied iteratively.</p>
<p>For stochastic gradient descent, instead of calculating the gradients over the whole training set, gradients are only calculated for an i.i.d. subsampeled &ldquo;minibatch&rdquo;.</p>
<p>There are further practical &ldquo;tricks&rdquo; to facilitate training, which shall be outlined shotly:</p>
<h3 id="minibatches">
  Minibatches
  <a class="anchor" href="#minibatches">#</a>
</h3>
<p>In this implementation a minibachsize of 265 was used, which seems realtively large to me, but provided the most stable results. In the paper, the the batchsize is not mentioned.</p>
<h3 id="learning-rate-schedule">
  Learning rate schedule
  <a class="anchor" href="#learning-rate-schedule">#</a>
</h3>
<p>Instead of using the same learning rate throughout, larger steps are taken in the beginning nad smaller steps in the end. This is done via learning rate schedule, decreasing the learning rate every X number of epochs.</p>
<h3 id="adam">
  ADAM
  <a class="anchor" href="#adam">#</a>
</h3>
<p>There are further improvements to stochastic gradient descent namely ADAM, which uses first and second order momentum to overcome plateaus. If you are not familiar, head over to Wikipedia TODO.</p>
<h1 id="talk-is-cheap-show-me-the-code---linus-torvalds">
  Talk is cheap, show me the code - Linus Torvalds
  <a class="anchor" href="#talk-is-cheap-show-me-the-code---linus-torvalds">#</a>
</h1>
<p>Luckily <code>pennylane</code> already comes with interfaces for lots of 
  <a href="https://pennylane.readthedocs.io/en/stable/introduction/optimizers.html">different opimizers</a>, such as NumPy, Pytorch and tensorflow.
As stated above, the ADAM optimizer from NumPy will be used.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> pennylane.optimize <span style="color:#f92672">import</span> AdamOptimizer

num_qubits <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>
num_layers <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>

qc<span style="color:#f92672">=</span> c19(num_layers)
num_params <span style="color:#f92672">=</span> qc<span style="color:#f92672">.</span>get_numparams()

best_val <span style="color:#f92672">=</span> (<span style="color:#ae81ff">0.01</span> <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(num_layers, num_params), <span style="color:#ae81ff">0.01</span>)
best_acc <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>

learning_rate <span style="color:#f92672">=</span> [<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">0.5</span>,<span style="color:#ae81ff">0.25</span>,<span style="color:#ae81ff">0.1</span>]
opt <span style="color:#f92672">=</span> AdamOptimizer(learning_rate, beta1<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, beta2<span style="color:#f92672">=</span><span style="color:#ae81ff">0.999</span>)
</code></pre></div><p>Furthermore, <code>pennylane</code> also comes with an array of 
  <a href="https://pennylane.readthedocs.io/en/stable/code/api/pennylane.QNode.html">differentiation methods</a>:</p>
<ul>
<li>
<p>&ldquo;best&rdquo;: Best available method. Uses classical backpropagation or the device directly to compute the gradient if supported, otherwise will use the analytic parameter-shift rule where possible with finite-difference as a fallback.</p>
</li>
<li>
<p>&ldquo;backprop&rdquo;: Use classical backpropagation. Only allowed on simulator devices that are classically end-to-end differentiable, for example default.tensor.tf. Note that the returned QNode can only be used with the machine learning framework supported by the device; a separate interface argument should not be passed.</p>
</li>
<li>
<p>&ldquo;device&rdquo;: Queries the device directly for the gradient. Only allowed on devices that provide their own gradient rules.</p>
</li>
<li>
<p>&ldquo;parameter-shift&rdquo;: Use the analytic parameter-shift rule where possible, with finite-difference as a fallback.</p>
</li>
<li>
<p>&ldquo;finite-diff&rdquo;: Uses numerical finite-differences for all parameters.</p>
</li>
<li>
<p>None: a non-differentiable QNode is returned.</p>
</li>
</ul>
<p>I have used the &ldquo;best method throughout, while the paper TODO uses SPSA.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">losslist <span style="color:#f92672">=</span> []

batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">256</span>

<span style="color:#75715e"># train the variational classifier</span>
var <span style="color:#f92672">=</span> best_val<span style="color:#75715e">#var_init</span>

<span style="color:#66d9ef">for</span> lr <span style="color:#f92672">in</span> learning_rate:
    opt <span style="color:#f92672">=</span> AdamOptimizer(lr, beta1<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, beta2<span style="color:#f92672">=</span><span style="color:#ae81ff">0.999</span>)

    <span style="color:#66d9ef">for</span> it <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">60</span>):

        <span style="color:#75715e"># Draw minibatch</span>
        batch_index <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>, num_train, (batch_size,))
        feats_train_batch <span style="color:#f92672">=</span> feats_train[batch_index]
        Y_train_batch <span style="color:#f92672">=</span> Y_train[batch_index]

        var <span style="color:#f92672">=</span> opt<span style="color:#f92672">.</span>step(<span style="color:#66d9ef">lambda</span> v: qc<span style="color:#f92672">.</span>cost(v, feats_train_batch, Y_train_batch), var)

        <span style="color:#75715e"># Compute predictions on train and validation set</span>
        predictions_train <span style="color:#f92672">=</span> [np<span style="color:#f92672">.</span>sign(qc<span style="color:#f92672">.</span>variational_classifier(var, angles<span style="color:#f92672">=</span>f)) <span style="color:#66d9ef">for</span> f <span style="color:#f92672">in</span> feats_train]
        predictions_val <span style="color:#f92672">=</span> [np<span style="color:#f92672">.</span>sign(qc<span style="color:#f92672">.</span>variational_classifier(var, angles<span style="color:#f92672">=</span>f)) <span style="color:#66d9ef">for</span> f <span style="color:#f92672">in</span> feats_val]

        <span style="color:#75715e"># Compute accuracy on train and validation set</span>
        acc_train <span style="color:#f92672">=</span> accuracy(Y_train, predictions_train)
        acc_val <span style="color:#f92672">=</span> accuracy(Y_val, predictions_val)

        <span style="color:#66d9ef">if</span> acc_train <span style="color:#f92672">&gt;</span> best_acc:
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;NEW BEST&#34;</span>)
            best_val <span style="color:#f92672">=</span> var
            best_acc <span style="color:#f92672">=</span> acc_train
            
        loss <span style="color:#f92672">=</span> qc<span style="color:#f92672">.</span>cost(var, features, Y)
        losslist<span style="color:#f92672">.</span>append(loss)

        <span style="color:#66d9ef">print</span>(
            <span style="color:#e6db74">&#34;Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} | Best acc: {:0.7f}&#34;</span>
            <span style="color:#e6db74">&#34;&#34;</span><span style="color:#f92672">.</span>format(it <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, loss, acc_train, acc_val, best_acc)
        )
</code></pre></div><h1 id="training-time">
  Training time
  <a class="anchor" href="#training-time">#</a>
</h1>
<p>Training a single datset on circuit19 took about 5 hours. For the 9 sets thats almost two whole days! Training the classical Neural network on a single dataset takes about 5 minutes&hellip;</p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>

 
        
      </footer>

      
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      
  <nav id="TableOfContents">
  <ul>
    <li><a href="#optimization">Optimization</a>
      <ul>
        <li><a href="#parameter-shift-rule">Parameter-shift rule</a></li>
        <li><a href="#spsa">SPSA</a></li>
        <li><a href="#stochastic-gradient-descent">(Stochastic) Gradient descent</a>
          <ul>
            <li><a href="#minibatches">Minibatches</a></li>
            <li><a href="#learning-rate-schedule">Learning rate schedule</a></li>
            <li><a href="#adam">ADAM</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#talk-is-cheap-show-me-the-code---linus-torvalds">Talk is cheap, show me the code - Linus Torvalds</a></li>
    <li><a href="#training-time">Training time</a></li>
  </ul>
</nav>

 
    </aside>
    
  </main>

  
</body>

</html>













'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/Table-of-contents/0.-Preface/','title':"0. Preface",'content':"What are Hybrid networks, anyway? #  While looking through the web, it seemed there is no clear-cut definition of \u0026ldquo;Hybrid\u0026rdquo;.\n  This Qiskit tutorial uses a classical Neural network to learn the optimal rotations for a simple quantum circuit.\n  This Tensorflow Quantum tutorial uses something akin to a regular max-pooling layer to first downproject the 28x28 images to 4x4 images, which are then fed into a qubit grid of same size.\n  One notable difference here being, whether the data is explicitly embedded into a quantum state, or if rotatations on a fixed initial state are optimized, representing the data implicitly.\n  Benedetti et al. define it in the following way:\n \u0026ldquo;Within the hybrid system, the quantum computer prepares quantum states according to a set of parameters. Using the measurement outcomes, the classical learning algorithm adjusts the parameters in order to minimize an objective function. The updated parameters, now defining a new quantumcircuit, are fed back to the quantum hardware in a closed loop.\u0026ldquo; 2\n Wikipedia introduces the distinction based on what realm of data is processed with which algorithm:\n  So, I guess in the end, any system that TODO\nSources #  1: arXiv:2003.09887  2: arXiv:1906.07682  https://commons.wikimedia.org/wiki/File:Qml_approaches.tif\n"});index.add({'id':1,'href':'/docs/Table-of-contents/1.-Introduction/','title':"1. Introduction",'content':"Environment setup #  There is actually a suprisingly large amount of different libraries for quantum state simulation. I will use pennylane by XANADU.\nQiskit\nTensorflow Quantum\nCirq\nVirtual env #  "});index.add({'id':2,'href':'/docs/Table-of-contents/2.-The-Data/','title':"2. the Data",'content':"About the Data #  This synthetic set consists of three subsets, each with three increasing difficulty levels, amounting to 9 datasets in total. Each of the nine sets consists of 1500 points (the amount is specified during the data generation). The data are split into 70% train and 30% validation.\n  Screenshot from  1\n*Note that the Datasets are nomalized to length one, but not centered around zero. To me it is unclear why that has not been done.\nSometimes the individual sets might be referenced as \u0026lsquo;1b\u0026rsquo; or \u0026lsquo;3c\u0026rsquo;, where the rows are indexed alphabetically top to bottom and the columns are numbered left to right.\nIn the following picture you can see the recreated data:\n  *Note that the exact coloring might not match with the originals, this does not matter as training is done seperately.\nIn 1a one can see the only overlapping and not linearly seperable dataset. Within the first row, there is an increase in the number of clusters and in thier respective bias towards the center. 1b is also imbalanced.\nThe second row shows a steady increase in number of clusters, with the middle one also being imbalanced.\nThe final row consists of enclosed and imbalanced datasets throughout, again with the number of clusters increasing left to right.\nSources #  1: arXiv:2003.09887 \n"});index.add({'id':3,'href':'/docs/Table-of-contents/3.-Architecture/','title':"3. Architecture",'content':"Ubi loqui #  Markdown\n  \\(Q \\)   "});index.add({'id':4,'href':'/docs/Table-of-contents/4.-Embedding/','title':"4. Embedding",'content':"How does one get the data into the Quantum circuit? #  There are a multitude of ways to do so. The two simplest ways to embed information are Amplitude and Product encoding. This embedding circuit preceeds all other quantum circuits shown later and will not be drawn explicitly.\nAmplitude encoding #  Amplitude encoding only works if the input features x are nomalized to   \\(\\bm{x} \\in [0,1] \\)  . Then the Quantum states are prepared as following:\nAmplitude encoding\n \\( x \\in\\{0,1\\}^{n} \\rightarrow |x \\rangle \\)   This of cause also works when working with binary input features, then it is called Basis encoding Amplitude embedding gives rise to a linear kernel in feature Hilbert space 3 and while it is consideres memory efficient, unfortunately ,the depth of this encoder circuit is expected to scale ex-ponentially with the number of qubits for generic inputs.Therefore, algorithms based on amplitude encoding couldbe impeded by our inability to coherently load data intoquantum states. 2\nProduct encoding #  Here, the initial states of the qubits are fixed, e.g. to  \\(|0\\rangle\\)  TODO\nProduct encoding\n \\( x \\in \\mathbb{R}^{N} \\rightarrow\\left|\\psi_{x}\\right\\rangle=\\cos \\left(x_{j}\\right)|0\\rangle\u0026#43;\\sin \\left(x_{j}\\right)|1\\rangle \\)   Embedding circuit #  Since the data is normalized and centered ( \\(\\bm{x} \\in [-0.5, 0.5] \\)  ) first a feature map  \\(\\phi(\\bm{x}) = 2*\\pi*\\bm{x} \\)  is applied.\n   \u0026ldquo;Each component of the feature-vector x is embedded as a single qubit rotation  \\( R_{x}(\\phi(x_{i}))\\)  with  \\(\\phi : x \\rightarrow [0,2\\pi)\\)  . As we work with two-dimensional data, we duplicate x0 and x1 on x2 and x3. After embedding the data in the parameters of the  \\(R_{x}\\)  gates, we apply a set of constant  \\(R_{y}\\)  and  \\(R_{z}\\)  rotations. That ensures that if projected down onto the computational basis states, the output states span approximately the same range.\u0026ldquo; 1\n Why the features are embedded twice #   \u0026ldquo;The no-cloning principle of quantum mechanics suggests that there is an advantage in redundantly encoding the input value severaltimes.\u0026ldquo; 4\n The Authors call this \u0026ldquo;input redundancy\u0026rdquo;, in our example the redundancy is n=2. Furthermore the authors conclude that\n \u0026ldquo;Input redundancy must be present if goodapproximations of functions are the goal\u0026rdquo;\n Code #  @qml.template def statepreparation(x): qml.RX(x[0], wires=0) qml.RX(x[1], wires=1) qml.RX(x[0], wires=2) qml.RX(x[1], wires=3) qml.RY(np.pi / 4, wires=0) qml.RY(np.pi / 4, wires=1) qml.RY(np.pi / 4, wires=2) qml.RY(np.pi / 4, wires=3) qml.RZ(np.pi / 4, wires=0) qml.RZ(np.pi / 4, wires=1) qml.RZ(np.pi / 4, wires=2) qml.RZ(np.pi / 4, wires=3) Sources #  1: arXiv:2003.09887  2: arXiv:1906.07682  3: Quantum machine learning in feature Hilbert spaces  4: Input Redundancy for Parameterized Quantum Circuits \n"});index.add({'id':5,'href':'/docs/Table-of-contents/5.-Parametrized-Quantum-Circuit/','title':"5. Parametrized Quantum Circuit",'content':"TODO #  The basic idea of quantum computing is similar to that of kernel methods in machine learning, to efficiently perform computations in an intractably large Hilbert/Kernel space.\nParameterized Quantum Circuits (PQCs, also sometimes referred to as \u0026ldquo;ansatz\u0026rdquo; or \u0026quot; variational quantum circuit\u0026rdquo;) are a combination of multiple quantum gates operating on one or multiple qubits. The quantum gates have free parameters which can be optimzied to fit a desired probability distribution.\nSimilar to the universal approximation theorem in neural networks, there always exists a quantum circuit that can represent a target function within an arbitrary small error. The caveat is that such a circuit may be exponentially deep and therefore impractical. 2\nThere are several ways to construct non-linear operations in quantumcircuits, both coherently (i.e., exploiting entanglement or measureing) or non-coherently (e.g., exploiting the natural coupling of the system to the environment (Noise)).\nThe circuits presented here and investigated in Hubregtsen et al. 1 were originally proposed by Sim et al. 5\nCircuit 1 #  Circuit number 1 consists of a set of Rx and Rz gates. It is not using any two qubit operations and therefore does not project the input state into a higher dimensional space. This circuit is charac-terized as having lower expressive power.\n  @qml.qnode(dev) def circuit_01(weights, angles=None): ###################### ##### CIRCUIT 01 ##### ###################### statepreparation(angles) for W in weights: qml.RX(W[0], wires=0) qml.RX(W[1], wires=1) qml.RX(W[2], wires=2) qml.RX(W[3], wires=3) qml.RZ(W[4], wires=0) qml.RZ(W[5], wires=1) qml.RZ(W[6], wires=2) qml.RZ(W[7], wires=3) return qml.probs(wires=[0, 1, 2, 3]) Muliples of these circuits can joined together, like layers in a neural network, where the output of the current layer serves as input for the next one. In the code snippet above, the number of layers is controlled by the number of dimensions in the parameter array W. However,the expressiveness of this circuit is not significantly increased when using a higher number of layers.\nCircuit 19 #  Circuit number 19 was constructed for the task of data classification. It consists of the same set of single rotation gates as circuit number 1 and is extended by a set of controlled Rx gates. The design criteria ofthe circuit is to generate a highly entangled state which allows an efficient projection of the data into a spacewhere it can be separated. Additionally, it has been shown that this circuit group can identify correlations in the input data.\n  @qml.qnode(dev) def circuit_19(weights, angles=None): ###################### ##### CIRCUIT 19 ##### ###################### statepreparation(angles) for W in weights: qml.RX(W[0], wires=0) qml.RX(W[1], wires=1) qml.RX(W[2], wires=2) qml.RX(W[3], wires=3) qml.RZ(W[4], wires=0) qml.RZ(W[5], wires=1) qml.RZ(W[6], wires=2) qml.RZ(W[7], wires=3) qml.CRX(W[8], wires=[3, 0]) qml.CRX(W[9], wires=[2, 3]) qml.CRX(W[10], wires=[1, 2]) qml.CRX(W[11], wires=[0, 1]) return qml.probs(wires=[0, 1, 2, 3]) Note that the performance of Circuit 19 dies increase with additional layers, as we will see in the results later.\nSources #  2: arXiv:1906.07682  5: Expressibility and entangling capability of parameterized quantum circuits for hybrid quantum-classical algorithms, arXiv:1905.10876 \n"});index.add({'id':6,'href':'/docs/Table-of-contents/6.-Measurement/','title':"6. Measurement",'content':"Ubi loqui #  Markdown\n  \\(Q\\)   "});index.add({'id':7,'href':'/docs/Table-of-contents/7.-Mapping/','title':"7. Mapping",'content':"Ubi loqui #  Markdown\n  \\(Q\\)   "});index.add({'id':8,'href':'/docs/Table-of-contents/8.-Loss/','title':"8. Loss",'content':"Ubi loqui #  Markdown\n  \\(Qz\\)   "});index.add({'id':9,'href':'/docs/Table-of-contents/9.-Optimization/','title':"9. Optimization",'content':"Ubi loqui #  Markdown\n  \\(Q \\)   "});index.add({'id':10,'href':'/docs/Table-of-contents/_10.-Results/','title':"10. Results",'content':"Ubi loqui #  Markdown\n  \\(Q \\)   "});index.add({'id':11,'href':'/docs/Table-of-contents/','title':"Table of Contents",'content':""});index.add({'id':12,'href':'/docs/','title':"Docs",'content':""});})();